{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de29e8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/mnt/v-jepa/jepa')\n",
    "sys.path.append('/mnt/v-jepa/jepa/decord/python')  # For decord\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from src.models.vision_transformer import VisionTransformer as VideoEncoder\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Define dataset\n",
    "class VideoDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_path, num_frames=16, frame_size=(224, 224)):\n",
    "        self.data_path = data_path\n",
    "        self.num_frames = num_frames\n",
    "        self.frame_size = frame_size\n",
    "        self.classes = sorted([d for d in os.listdir(data_path) if os.path.isdir(os.path.join(data_path, d))])\n",
    "        self.class_to_idx = {cls: idx for idx, cls in enumerate(self.classes)}\n",
    "        self.videos = []\n",
    "        for cls in self.classes:\n",
    "            cls_path = os.path.join(data_path, cls)\n",
    "            for video in os.listdir(cls_path):\n",
    "                if video.endswith(('.mp4', '.avi')):\n",
    "                    self.videos.append((os.path.join(cls_path, video), cls))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.videos)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        video_path, cls = self.videos[idx]\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        if total_frames < self.num_frames:\n",
    "            frame_indices = [i % total_frames for i in range(self.num_frames)]\n",
    "        else:\n",
    "            frame_indices = np.linspace(0, total_frames - 1, self.num_frames, dtype=int)\n",
    "        frames = []\n",
    "        for idx in frame_indices:\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                frame = frames[-1] if frames else np.zeros((*self.frame_size, 3), dtype=np.uint8)\n",
    "            frame = cv2.resize(frame, self.frame_size)\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            frame = torch.tensor(frame).permute(2, 0, 1).float() / 255.0\n",
    "            frames.append(frame)\n",
    "        cap.release()\n",
    "        frames = torch.stack(frames)\n",
    "        label = torch.tensor(self.class_to_idx[cls], dtype=torch.long)\n",
    "        return {'frames': frames, 'label': label, 'path': video_path}\n",
    "\n",
    "dataset = VideoDataset(data_path=\"/path/to/dataset/\")\n",
    "loader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=True, num_workers=0)\n",
    "\n",
    "# Load pre-trained model\n",
    "encoder = VideoEncoder(\n",
    "    model_name='vit_large_patch16_224',\n",
    "    num_frames=16,\n",
    "    tubelet_size=1, # Process each frame individually\n",
    "    img_size=224,\n",
    "    patch_size=16,\n",
    "    embed_dim=1024, # ViT-Large\n",
    "    depth=24,\n",
    "    num_heads=16\n",
    ")\n",
    "checkpoint = torch.load('/mnt/v-jepa/jepa/vitl16.pth.tar', map_location='cuda')\n",
    "encoder.load_state_dict(checkpoint['encoder'], strict=False)\n",
    "encoder.cuda().train()\n",
    "\n",
    "# Add classification head\n",
    "num_classes = len(dataset.classes)\n",
    "classifier = nn.Linear(encoder.embed_dim, num_classes).cuda().train()\n",
    "\n",
    "# Optimizer and loss\n",
    "optimizer = torch.optim.Adam(list(encoder.parameters()) + list(classifier.parameters()), lr=1e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Fine-tuning loop\n",
    "for epoch in range(20):\n",
    "    for batch in loader:\n",
    "        frames = batch['frames'].cuda() # [B, T, C, H, W]\n",
    "        frames = frames.permute(0, 2, 1, 3, 4) # [B, C, T, H, W]\n",
    "        labels = batch['label'].cuda()\n",
    "        optimizer.zero_grad()\n",
    "        repr = encoder(frames)\n",
    "        logits = classifier(repr.mean(dim=1))\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n",
    "torch.save({'encoder': encoder.state_dict(), 'classifier': classifier.state_dict()}, '/mnt/v-jepa/jepa/finetuned_vitl16_20e.pth')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
